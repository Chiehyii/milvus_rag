import os
import psycopg2
import time
import json
from dotenv import load_dotenv

from openai import OpenAI
from pymilvus import MilvusClient
from intent_classification import intent_classification

# Load environment variables from .env file
load_dotenv()

zilliz_api_key = os.getenv("ZILLIZ_API_KEY")
api_key = os.getenv("OPENAI_API_KEY")
openai_client = OpenAI(api_key=api_key)
CLUSTER_ENDPOINT="https://in03-a6f08ce2ff778ed.serverless.gcp-us-west1.cloud.zilliz.com:443"
milvus_client = MilvusClient(
                    uri=CLUSTER_ENDPOINT,
                    token=zilliz_api_key,
                    )
collection_name = "rag5_scholarships_hybrid"

def get_embedding(text):
    """ç”¢ç”Ÿæ–‡å­—å‘é‡"""
    resp = openai_client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return resp.data[0].embedding

def retrieve_context(question: str, top_k: int=7):
    """æ ¹æ“šå•é¡Œé€²è¡Œç›¸ä¼¼åº¦æª¢ç´¢+éæ¿¾"""
    # 1. ç”¢ç”Ÿå•é¡Œçš„å‘é‡
    question_embedding = get_embedding(question)

    # 2. é‡å•é¡Œä¸­æå– metadata éæ¿¾æ¢ä»¶
    # filters = extract_filters_from_question(question)
    # expr = filters_to_expr(filters) if filters else None
    # print("Milvus expr:", expr)

    # 3. åŸ·è¡Œå‘é‡æª¢ç´¢
    search_params = {
        "metric_type": "COSINE",
        "params": {"nprobe": 10} # ç¢ºä¿é€™è£¡æ²’æœ‰ "expr"
    }

    results = milvus_client.search(
        collection_name=collection_name,
        data=[question_embedding],
        search_params=search_params,
        limit=top_k,
        # filter=expr if expr else None,
        output_fields=["id", "text", "source_file", "source_url", "status", "subsidy_type", "edu_system"],
    )
    if not results or not results[0]:
        return []

    return results[0]

def log_and_clean_contexts(retrieved_docs: list):
    """
    å°‡æª¢ç´¢çµæœæ‰“å°åˆ°æ§åˆ¶å°ï¼Œä¸¦è¿”å›ä¸€å€‹æ¸…ç†éçš„ã€å¯åºåˆ—åŒ–çš„åˆ—è¡¨ã€‚
    """
    print("\n=== RAGæª¢ç´¢çµæœ ===")
    if not retrieved_docs:
        print("ï¼ˆæ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•çµæœï¼‰")
        return []

    cleaned_contexts = []
    for i, res in enumerate(retrieved_docs, 1):
        entity = res.get("entity", {})

        # æ‰“å°æ—¥èªŒ
        print(f"çµæœ {i}:")
        print(f"å…§å®¹: {entity.get('text', '')[:100]}...")
        print(f"ç›¸ä¼¼åº¦: {res.get('distance', 0.0):.4f}, ä¾†æº: {entity.get('source_file', 'N/A')}")
        print("-" * 50)

        # æº–å‚™æ¸…ç†éçš„è³‡æ–™
        # å¾ Milvus ç²å–çš„ ARRAY é¡å‹å¯èƒ½æ˜¯ç„¡æ³•ç›´æ¥åºåˆ—åŒ–çš„ Protobuf é¡å‹/å®¹å™¨,
        # åœ¨æ­¤è™•æ‰‹å‹•è½‰æ›ç‚º python list
        status = entity.get("status")
        subsidy_type = entity.get("subsidy_type")
        edu_system = entity.get("edu_system")

        cleaned_contexts.append({
            "id": res.get("id"),
            "text": entity.get("text"),
            "source_file": entity.get("source_file", "").replace(".md", ""),
            "source_url": entity.get("source_url"),
            "status": list(status) if status else [],
            "subsidy_type": list(subsidy_type) if subsidy_type else [],
            "edu_system": list(edu_system) if edu_system else [],
            "distance": res.get("distance")
        })

    return cleaned_contexts

# ------------------------------------------------- ç”Ÿæˆç­”æ¡ˆ--------------------------------------------------
def generate_answer(question: str, cleaned_contexts: list):
    """æŠŠæ¸…ç†éçš„ Milvus æª¢ç´¢çµæœäº¤çµ¦ GPT ç”Ÿæˆè‡ªç„¶èªè¨€å›ç­”ï¼Œä¸¦è¿”å›å®Œæ•´çš„ API å›æ‡‰"""

    # ... (The existing logic for preparing context_for_llm remains the same)
    from collections import defaultdict
    grouped = defaultdict(list)
    source_url_map = {}
    for c in cleaned_contexts:
        fname = c.get('source_file', 'æœªçŸ¥ä¾†æº')
        grouped[fname].append(c.get('text', ''))
        if fname not in source_url_map and c.get('source_url'):
            source_url_map[fname] = c.get('source_url')

    context_for_llm = ""
    for fname, texts in grouped.items():
        title = fname.replace('.md', '').replace('.txt', '')
        url = source_url_map.get(fname, '')
        context_for_llm += f"\n---\nä¾†æºåç¨±: {title}\n"
        if url:
            context_for_llm += f"ä¾†æºç¶²å€: {url}\n"
        full_text = "\n".join(texts)
        context_for_llm += f"å…§å®¹: {full_text}\n"

    system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ…ˆæ¿Ÿå¤§å­¸çå­¸é‡‘å•ç­”åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ã€Œæª¢ç´¢å…§å®¹ã€ä¾†å›ç­”ã€Œä½¿ç”¨è€…å•é¡Œã€ã€‚

    **è¼¸å‡ºæ ¼å¼**
    ä½ çš„è¼¸å‡ºå¿…é ˆåš´æ ¼åŒ…å«å…©éƒ¨åˆ†ï¼Œä¸¦ç”±ä¸€å€‹ç‰¹æ®Šçš„åˆ†éš”ç¬¦è™Ÿ `|||SOURCES|||` éš”é–‹ã€‚

    **ç¬¬ä¸€éƒ¨åˆ†ï¼šçµ¦ä½¿ç”¨è€…çš„å›ç­”**
    1.  **åˆ†æ**ï¼šä»”ç´°åˆ†æã€Œæª¢ç´¢å…§å®¹ã€ï¼Œåˆ¤æ–·å“ªäº›ä¾†æºèˆ‡ã€Œä½¿ç”¨è€…å•é¡Œã€çœŸæ­£ç›¸é—œã€‚
    2.  **ç”Ÿæˆå›ç­”**ï¼š
        * å¦‚æœæœ‰å¤šå€‹çåŠ©å­¸é‡‘ç¨®é¡å°±ç‚ºæ¯å€‹çå­¸é‡‘æˆ–è£œåŠ©å»ºç«‹ä¸€å€‹ç¨ç«‹çš„æ®µè½ã€‚
        * åªå›ç­”å’Œã€Œä½¿ç”¨è€…å•é¡Œã€ç›´æ¥ç›¸é—œçš„è³‡è¨Šï¼Œé¿å…åŒ…å«ä¸ç›¸é—œçš„ç´°ç¯€ã€‚
        * å¦‚æœã€Œæª¢ç´¢å…§å®¹ã€ä¸­æ²’æœ‰ä»»ä½•è³‡è¨Šèƒ½å›ç­”ã€Œä½¿ç”¨è€…å•é¡Œã€ï¼Œè«‹ç¦®è²Œåœ°å‘ŠçŸ¥ä½¿ç”¨è€…ä½ ç„¡æ³•å›ç­”ï¼Œè€Œä¸æ˜¯ç·¨é€ è³‡è¨Šã€‚
        * æ¯å€‹æ®µè½éƒ½å¿…é ˆä»¥åˆ†é»åˆ—å‡ºï¼Œä¸¦å¿…é ˆéµå¾ªä»¥ä¸‹æ ¼å¼ç¨ç«‹å‘ˆç¾ï¼š
            * æ¨™é¡Œï¼šè©²çå­¸é‡‘çš„ã€Œä¾†æºåç¨±ã€ä½œç‚ºæ¨™é¡Œï¼ˆä½¿ç”¨ Markdown çš„ `**ç²—é«”**` æ ¼å¼ï¼‰ã€‚
            * å…§å®¹ï¼šæ ¹æ“šæª¢ç´¢å…§å®¹ä¸­ï¼Œä»¥æµæš¢çš„æ®µè½æˆ–é …ç›®ç¬¦è™Ÿä¾†å‘ˆç¾ã€‚
        * åœ¨æ¨™é¡Œä¸‹æ–¹ï¼Œåƒ…ä½¿ç”¨ç›¸é—œçš„å…§å®¹ä¾†çµ„ç¹”ä½ çš„å›ç­”ã€‚
        * ä½¿ç”¨è‡ªç„¶çš„èªè¨€å’Œ Markdown æ’ç‰ˆï¼ˆç²—é«”ã€é …ç›®ç¬¦è™Ÿç­‰ï¼‰ä¾†ç¾åŒ–è¼¸å‡ºã€‚
    3.  **ç¦æ­¢**ï¼šä¸è¦åœ¨é€™éƒ¨åˆ†åŒ…å«ä»»ä½•é—œæ–¼è³‡æ–™ä¾†æºçš„æ–‡å­—ï¼ˆæ¨™é¡Œé™¤å¤–ï¼‰ã€‚

    **ç¬¬äºŒéƒ¨åˆ†ï¼šè³‡æ–™ä¾†æºåˆ—è¡¨**
    1.  åœ¨åˆ†éš”ç¬¦è™Ÿ `|||SOURCES|||` ä¹‹å¾Œï¼Œä½ å¿…é ˆåˆ—å‡ºä½ åœ¨ç¬¬ä¸€éƒ¨åˆ†å›ç­”ä¸­ï¼Œæ‰€ä½¿ç”¨åˆ°çš„æ‰€æœ‰ã€Œä¾†æºåç¨±ã€ã€‚
    2.  æ ¼å¼ç‚ºä¸€å€‹ç°¡å–®çš„ã€ç”±é€—è™Ÿåˆ†éš”çš„å­—ä¸²ï¼Œä¾‹å¦‚ï¼š`ä¾†æºåç¨±ä¸€,ä¾†æºåç¨±äºŒ`ã€‚
    3.  å¦‚æœæ ¹æ“šã€Œæª¢ç´¢å…§å®¹ã€ç„¡æ³•å›ç­”å•é¡Œï¼Œå‰‡é€™éƒ¨åˆ†æ‡‰ç‚ºç©ºã€‚

    """

    user_prompt = f"""
    ä½¿ç”¨è€…å•é¡Œï¼š
    {question}

    æª¢ç´¢å…§å®¹ï¼š
    {context_for_llm}
    """

    # è¿”å›å®Œæ•´çš„ response ç‰©ä»¶
    return openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.0,
    )

def log_to_db(question, rephrased_question, answer, contexts, latency_ms, usage):
    """å°‡å•ç­”è³‡æ–™å’Œ token ä½¿ç”¨é‡è¨˜éŒ„åˆ° PostgreSQL è³‡æ–™åº«ä¸­"""
    conn = None
    cursor = None
    try:
        # å¾ç’°å¢ƒè®Šæ•¸è®€å– PostgreSQL é€£ç·šè³‡è¨Š
        conn = psycopg2.connect(
            host=os.getenv("DB_HOST", "localhost"),
            port=os.getenv("DB_PORT", "5432"),
            dbname=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )
        cursor = conn.cursor()
        
        # ç¢ºä¿ retrieved_contexts æ˜¯åˆæ³•çš„ JSON å­—ä¸²
        # psycopg2 æœƒè‡ªå‹•è™•ç† Python dict åˆ° JSONB çš„è½‰æ›
        
        # å¾ usage ç‰©ä»¶ä¸­å®‰å…¨åœ°ç²å– token è³‡è¨Š
        prompt_tokens = usage.prompt_tokens if usage else None
        completion_tokens = usage.completion_tokens if usage else None
        total_tokens = usage.total_tokens if usage else None

        TABLE_NAME = "qa_logs"
        insert_query = f"""INSERT INTO {TABLE_NAME} 
                         (question, rephrased_question, answer, retrieved_contexts, latency_ms, prompt_tokens, completion_tokens, total_tokens)
                         VALUES (%s, %s, %s, %s, %s, %s, %s, %s)"""
        
        # å°‡ contexts python dict ç›´æ¥å‚³éï¼Œpsycopg2 æœƒå°‡å…¶åºåˆ—åŒ–ç‚º JSON
        cursor.execute(insert_query, (question, rephrased_question, answer, json.dumps(contexts, ensure_ascii=False), latency_ms, prompt_tokens, completion_tokens, total_tokens))
        conn.commit()
        print("\n[DB] æœ¬æ¬¡å•ç­”ç´€éŒ„å·²æˆåŠŸå„²å­˜åˆ° PostgreSQL è³‡æ–™åº«ã€‚")

    except psycopg2.Error as e:
        print(f"\n[DB Error] ç„¡æ³•å¯«å…¥ PostgreSQL è³‡æ–™åº«: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def _rephrase_question_with_history(history: list, question: str) -> str:
    """
    ä½¿ç”¨å°è©±æ­·å²ä¾†é‡æ§‹ä¸€å€‹æ–°çš„ã€ç¨ç«‹çš„å•é¡Œã€‚
    """
    if not history:
        return question

    # å°‡ history è½‰æ›ç‚ºé©åˆ LLM çš„æ ¼å¼
    # ç‚ºé¿å…éé•·ï¼Œåªå–æœ€è¿‘çš„ 4 è¼ªå°è©±
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-8:]])

    system_prompt = """ä½ æ˜¯ä¸€å€‹å°è©±åŠ©ç†ï¼Œä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ã€Œå°è©±æ­·å²ã€å’Œã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€ï¼Œç”Ÿæˆä¸€å€‹ç¨ç«‹ã€å®Œæ•´çš„ã€Œé‡æ§‹å¾Œçš„å•é¡Œã€ã€‚
é€™å€‹ã€Œé‡æ§‹å¾Œçš„å•é¡Œã€å¿…é ˆèƒ½å¤ åœ¨æ²’æœ‰ä»»ä½•ä¸Šä¸‹æ–‡çš„æƒ…æ³ä¸‹è¢«å®Œå…¨ç†è§£ã€‚

**è¦å‰‡:**
- å¦‚æœã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€**ä¸æ˜¯ä¸€å€‹å•é¡Œ** (ä¾‹å¦‚ï¼šé“è¬ "è¬è¬", è‚¯å®š "æˆ‘çŸ¥é“äº†", å•å€™ "ä½ å¥½"), **è«‹ç›´æ¥åŸæ¨£è¿”å›ã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€**ï¼Œä¸è¦åšä»»ä½•æ”¹å¯«ã€‚
- å¦‚æœã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€å·²ç¶“æ˜¯ä¸€å€‹å®Œæ•´çš„ã€å¯ç¨ç«‹ç†è§£çš„å•é¡Œï¼Œç›´æ¥è¿”å›åŸå•é¡Œã€‚
- å¦å‰‡ï¼Œè«‹çµåˆã€Œå°è©±æ­·å²ã€ä¾†æ”¹å¯«å•é¡Œï¼Œä½¿å…¶è®Šå¾—å®Œæ•´ã€‚
- ä¿æŒå•é¡Œç°¡æ½”ã€‚

ä¾‹å¦‚ (éœ€è¦æ”¹å¯«):
å°è©±æ­·å²:
user: æˆ‘æƒ³æ‰¾æ¸…å¯’çå­¸é‡‘
assistant: æˆ‘å€‘æœ‰å¹¾ç¨®æ¸…å¯’çå­¸é‡‘ï¼Œä¾‹å¦‚ A å’Œ Bã€‚
æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œ:
å®ƒéœ€è¦ä»€éº¼è³‡æ ¼?

é‡æ§‹å¾Œçš„å•é¡Œ:
ç”³è«‹ B æ¸…å¯’çå­¸é‡‘éœ€è¦ä»€éº¼è³‡æ ¼ï¼Ÿ

ä¾‹å¦‚ (ç„¡éœ€æ”¹å¯«):
å°è©±æ­·å²:
user: æ…ˆæ¿Ÿé†«ç™‚æ³•äººçåŠ©å­¸é‡‘çš„ç”³è«‹æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ
assistant: ç”³è«‹æµç¨‹æ˜¯...
æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œ:
è¬è¬

é‡æ§‹å¾Œçš„å•é¡Œ:
è¬è¬
"""

    user_prompt = f"""å°è©±æ­·å²:
{history_str}

æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œ:
{question}

é‡æ§‹å¾Œçš„å•é¡Œ:
"""
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
            max_tokens=150, # é™åˆ¶è¼¸å‡ºé•·åº¦
        )
        rephrased_question = response.choices[0].message.content.strip()
        # é¿å…è¿”å›ç©ºå­—ä¸²
        if not rephrased_question:
            return question
        print(f"ğŸ”„ é‡æ§‹å¾Œçš„å•é¡Œ: {rephrased_question}")
        return rephrased_question
    except Exception as e:
        print(f"âš ï¸ å•é¡Œé‡æ§‹å¤±æ•—: {e}")
        return question # Fallback to the original question

def chat_pipeline(question: str, history: list | None = None):
    start_time = time.time()
    result = {}
    usage = None
    original_question = question # ä¿å­˜åŸå§‹å•é¡Œä»¥ä¾›æ—¥èªŒè¨˜éŒ„
    contexts_for_logging = [] # ç”¨æ–¼å„²å­˜å®Œæ•´çš„ã€æœªç¶“éå»é‡çš„ä¸Šä¸‹æ–‡ï¼Œä»¥ä¾¿æ—¥èªŒè¨˜éŒ„

    try:
        # å¦‚æœæœ‰æ­·å²ç´€éŒ„ï¼Œé‡æ§‹å•é¡Œ
        if history:
            question = _rephrase_question_with_history(history, question)
        
        print(f"\nâ“ æœ€çµ‚å•é¡Œ: {question} (åŸå§‹: {original_question})")

        intent = intent_classification(question)
        print(f"æ„åœ–: {intent}")

        if intent == "scholarship":
            raw_contexts = retrieve_context(question)
            cleaned_contexts = log_and_clean_contexts(raw_contexts)

            if not cleaned_contexts:
                result = {"answer": "æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„è£œåŠ©æˆ–çå­¸é‡‘è³‡è¨Šã€‚","contexts":[]}
                contexts_for_logging = [] # ç¢ºä¿åœ¨è¿”å›å‰è³¦å€¼
                return result
            
            # Step 4: ç²å–å®Œæ•´çš„ API å›æ‡‰
            llm_response = generate_answer(question, cleaned_contexts)
            llm_output = llm_response.choices[0].message.content.strip()
            usage = llm_response.usage # ä¿å­˜ usage ç‰©ä»¶

            # Step 5: è§£æ LLM è¼¸å‡º
            answer = llm_output
            cited_source_names = []
            if "|||SOURCES|||" in llm_output:
                parts = llm_output.split("|||SOURCES|||")
                answer = parts[0].strip()
                source_names_str = parts[1].strip()
                if source_names_str:
                    cited_source_names = [name.strip() for name in source_names_str.split(',')]

            # Step 6: éæ¿¾å‡ºå®Œæ•´çš„å¼•ç”¨ä¸Šä¸‹æ–‡ï¼Œç”¨æ–¼æ—¥èªŒè¨˜éŒ„
            cited_source_names_set = set(cited_source_names)
            all_cited_contexts = []
            for context in cleaned_contexts:
                if context.get('source_file') in cited_source_names_set:
                    all_cited_contexts.append(context)
            
            contexts_for_logging = all_cited_contexts # å°‡å®Œæ•´åˆ—è¡¨è³¦å€¼çµ¦æ—¥èªŒå°ˆç”¨è®Šæ•¸

            # Step 7: å»ºç«‹ä¸€å€‹å»é‡çš„ç‰ˆæœ¬ï¼Œç”¨æ–¼å‰ç«¯é¡¯ç¤º
            unique_display_contexts = []
            seen_keys = set()
            for context in all_cited_contexts:
                # å„ªå…ˆä½¿ç”¨ URL ä½œç‚ºå”¯ä¸€æ¨™è­˜ï¼Œè‹¥ç„¡å‰‡ä½¿ç”¨æª”å
                unique_key = context.get('source_url') or context.get('source_file')
                if unique_key not in seen_keys:
                    unique_display_contexts.append(context)
                    seen_keys.add(unique_key)
            
            result = {"answer": answer, "contexts": unique_display_contexts} # å›å‚³çµ¦å‰ç«¯çš„æ˜¯å»é‡å¾Œçš„ç‰ˆæœ¬

            print(f"ğŸ’¡ LLM å›ç­”: {result['answer']}")
            if result["contexts"]:
                print("\n--- LLM å¯¦éš›åƒè€ƒä¾†æº ---")
                for i, context in enumerate(result["contexts"], 1):
                    print(f"{i}. {context.get('source_file', 'N/A')}")
                print("-------------------------")

            return result

        else:
            # å°æ–¼é–’èŠï¼ŒåŒæ¨£ç²å–å®Œæ•´å›æ‡‰
            resp = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ…ˆæ¿Ÿå¤§å­¸çš„èŠå¤©åŠ©ç†ï¼Œä¸»è¦æä¾›çåŠ©å­¸é‡‘å’Œè£œåŠ©è³‡è¨Šã€‚è«‹è‡ªç„¶ä¸”ç°¡çŸ­åœ°å›æ‡‰ï¼Œä¸¦å¼•å°ä½¿ç”¨è€…æå•ç›¸é—œå•é¡Œã€‚è‹¥å•é¡Œç„¡é—œï¼Œè«‹ç¦®è²Œåœ°è¡¨ç¤ºç„¡æ³•å›ç­”ã€‚"},
                    {"role": "user", "content": question}
                ],
                temperature=0.7
            )
            usage = resp.usage # ä¿å­˜ usage ç‰©ä»¶
            result = {"answer": resp.choices[0].message.content.strip(), "contexts": []}
            contexts_for_logging = [] # ç¢ºä¿åœ¨è¿”å›å‰è³¦å€¼
            print(f"ğŸ’¡ LLM å›ç­”: {result['answer']}")
            return result
    finally:
        end_time = time.time()
        latency_ms = (end_time - start_time) * 1000
        print(f"\nâ±ï¸ æœ¬æ¬¡å•ç­”ç¸½è€—æ™‚: {latency_ms:.2f} ms")
        
        final_answer = result.get("answer", "")
        
        # ä½¿ç”¨å°ˆé–€ç‚ºæ—¥èªŒæº–å‚™çš„ã€æœªç¶“éå»é‡çš„å®Œæ•´ä¸Šä¸‹æ–‡åˆ—è¡¨
        log_to_db(original_question, question, final_answer, contexts_for_logging, latency_ms, usage)

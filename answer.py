import sys
import psycopg2
import time
import json
import asyncio

from openai import AsyncOpenAI
from pymilvus import MilvusClient

# åŒ¯å…¥é›†ä¸­åŒ–çš„è¨­å®š
import config

from auto_filter import extract_filters_from_question, filters_to_expr
from intent_classification import intent_classification

# ä½¿ç”¨é›†ä¸­åŒ–çš„è¨­å®šä¾†åˆå§‹åŒ– clients
openai_client = AsyncOpenAI(api_key=config.OPENAI_API_KEY)
milvus_client = MilvusClient(
    uri=config.CLUSTER_ENDPOINT,
    token=config.ZILLIZ_API_KEY,
)

async def get_embedding(text):
    """ç”¢ç”Ÿæ–‡å­—å‘é‡"""
    resp = await openai_client.embeddings.create(
        input=text,
        model=config.EMBEDDING_MODEL
    )
    return resp.data[0].embedding

async def retrieve_context(question: str, top_k: int=7):
    """æ ¹æ“šå•é¡Œé€²è¡Œç›¸ä¼¼åº¦æª¢ç´¢+éæ¿¾"""
    # 1. ç”¢ç”Ÿå•é¡Œçš„å‘é‡
    question_embedding = await get_embedding(question)

    # 2. é‡å•é¡Œä¸­æå– metadata éæ¿¾æ¢ä»¶
    filters = extract_filters_from_question(question)
    expr = filters_to_expr(filters) if filters else None
    print("Milvus expr:", expr)

    # 3. åŸ·è¡Œå‘é‡æª¢ç´¢
    search_params = {
        "metric_type": "COSINE",
        "params": {"nprobe": 10} # ç¢ºä¿é€™è£¡æ²’æœ‰ "expr"
    }

    # milvus_client.search is synchronous; run in a thread to avoid blocking the event loop
    def _milvus_search():
        return milvus_client.search(
            collection_name=config.MILVUS_COLLECTION,
            data=[question_embedding],
            search_params=search_params,
            limit=top_k,
            filter=expr if expr else None,
            output_fields=["id", "text", "source_file", "source_url", "status", "subsidy_type", "edu_system"],
        )

    results = await asyncio.to_thread(_milvus_search)
    if not results or not results[0]:
        return []

    return results[0]

def log_and_clean_contexts(retrieved_docs: list):
    """
    å°‡æª¢ç´¢çµæœæ‰“å°åˆ°æ§åˆ¶å°ï¼Œä¸¦è¿”å›ä¸€å€‹æ¸…ç†éçš„ã€å¯åºåˆ—åŒ–çš„åˆ—è¡¨ã€‚
    """
    print("\n=== RAGæª¢ç´¢çµæœ ===")
    if not retrieved_docs:
        print("ï¼ˆæ²’æœ‰æª¢ç´¢åˆ°ä»»ä½•çµæœï¼‰")
        return []

    cleaned_contexts = []
    for i, res in enumerate(retrieved_docs, 1):
        entity = res.get("entity", {})

        # æ‰“å°æ—¥èªŒ
        print(f"çµæœ {i}:")
        print(f"å…§å®¹: {entity.get('text', '')[:100]}...")
        print(f"ç›¸ä¼¼åº¦: {res.get('distance', 0.0):.4f}, ä¾†æº: {entity.get('source_file', 'N/A')}")
        print("-" * 50)

        # æº–å‚™æ¸…ç†éçš„è³‡æ–™
        # å¾ Milvus ç²å–çš„ ARRAY é¡å‹å¯èƒ½æ˜¯ç„¡æ³•ç›´æ¥åºåˆ—åŒ–çš„ Protobuf é¡å‹/å®¹å™¨,
        # åœ¨æ­¤è™•æ‰‹å‹•è½‰æ›ç‚º python list
        status = entity.get("status")
        subsidy_type = entity.get("subsidy_type")
        edu_system = entity.get("edu_system")

        cleaned_contexts.append({
            "id": res.get("id"),
            "text": entity.get("text"),
            "source_file": entity.get("source_file", "").replace(".md", ""),
            "source_url": entity.get("source_url"),
            "status": list(status) if status else [],
            "subsidy_type": list(subsidy_type) if subsidy_type else [],
            "edu_system": list(edu_system) if edu_system else [],
            "distance": res.get("distance")
        })

    return cleaned_contexts


def log_to_db(question, rephrased_question, answer, contexts, latency_ms, usage):
    """å°‡å•ç­”è³‡æ–™å’Œ token ä½¿ç”¨é‡è¨˜éŒ„åˆ° PostgreSQL è³‡æ–™åº«ä¸­"""
    conn = None
    cursor = None
    try:
        # # ä½¿ç”¨ config è£¡çš„é€£ç·šè³‡è¨Š
        conn = psycopg2.connect(
            host=config.DB_HOST,
            port=config.DB_PORT,
            dbname=config.DB_NAME,
            user=config.DB_USER,
            password=config.DB_PASSWORD
        )
        # db_url = os.getenv("DATABASE_URL")
        # if not db_url:
        #     raise ValueError("\n[DB Error]âŒ DATABASE_ environment variable not set.")
        
        # conn = psycopg2.connect(db_url)
        cursor = conn.cursor()
        
        # ç¢ºä¿ retrieved_contexts æ˜¯åˆæ³•çš„ JSON å­—ä¸²
        # psycopg2 æœƒè‡ªå‹•è™•ç† Python dict åˆ° JSONB çš„è½‰æ›
        
        # å¾ usage ç‰©ä»¶ä¸­å®‰å…¨åœ°ç²å– token è³‡è¨Š
        prompt_tokens = usage.prompt_tokens if usage else None
        completion_tokens = usage.completion_tokens if usage else None
        total_tokens = usage.total_tokens if usage else None

        insert_query = f"""INSERT INTO {config.DB_TABLE_NAME} 
                         (question, rephrased_question, answer, retrieved_contexts, latency_ms, prompt_tokens, completion_tokens, total_tokens)
                         VALUES (%s, %s, %s, %s, %s, %s, %s, %s) RETURNING id;"""
        
        # å°‡ contexts python dict ç›´æ¥å‚³éï¼Œpsycopg2 æœƒå°‡å…¶åºåˆ—åŒ–ç‚º JSON
        cursor.execute(insert_query, (question, rephrased_question, answer, json.dumps(contexts, ensure_ascii=False), latency_ms, prompt_tokens, completion_tokens, total_tokens))
        
        # ç²å–è¿”å›çš„ id
        log_id = cursor.fetchone()[0]
        
        conn.commit()
        print(f"\n[DB] æœ¬æ¬¡å•ç­”ç´€éŒ„å·²æˆåŠŸå„²å­˜åˆ° PostgreSQL è³‡æ–™åº«ï¼ŒID: {log_id}ã€‚")
        return log_id

    except psycopg2.Error as e:
        print(f"\n[DB Error] ç„¡æ³•å¯«å…¥ PostgreSQL è³‡æ–™åº«: {e}")
        return None
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

async def _rephrase_question_with_history(history: list, question: str) -> str:
    """
    ä½¿ç”¨å°è©±æ­·å²ä¾†é‡æ§‹ä¸€å€‹æ–°çš„ã€ç¨ç«‹çš„å•é¡Œã€‚
    """
    if not history:
        return question

    # å°‡ history è½‰æ›ç‚ºé©åˆ LLM çš„æ ¼å¼
    # ç‚ºé¿å…éé•·ï¼Œåªå–æœ€è¿‘çš„ 4 è¼ªå°è©±
    history_str = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-8:]])

    system_prompt = """ä½ æ˜¯ä¸€å€‹å°è©±åŠ©ç†ï¼Œä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ã€Œå°è©±æ­·å²ã€å’Œã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€ï¼Œç”Ÿæˆä¸€å€‹ç¨ç«‹ã€å®Œæ•´çš„ã€Œé‡æ§‹å¾Œçš„å•é¡Œã€ã€‚
    é€™å€‹ã€Œé‡æ§‹å¾Œçš„å•é¡Œã€å¿…é ˆèƒ½å¤ åœ¨æ²’æœ‰ä»»ä½•ä¸Šä¸‹æ–‡çš„æƒ…æ³ä¸‹è¢«å®Œå…¨ç†è§£ã€‚

    **è¦å‰‡:**
    - å¦‚æœã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€**ä¸æ˜¯ä¸€å€‹å•é¡Œ** (ä¾‹å¦‚ï¼šé“è¬ "è¬è¬", è‚¯å®š "æˆ‘çŸ¥é“äº†", å•å€™ "ä½ å¥½"), **è«‹ç›´æ¥åŸæ¨£è¿”å›ã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€**ï¼Œä¸è¦åšä»»ä½•æ”¹å¯«ã€‚
    - å¦‚æœã€Œæœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œã€å·²ç¶“æ˜¯ä¸€å€‹å®Œæ•´çš„ã€å¯ç¨ç«‹ç†è§£çš„å•é¡Œï¼Œç›´æ¥è¿”å›åŸå•é¡Œã€‚
    - å¦å‰‡ï¼Œè«‹çµåˆã€Œå°è©±æ­·å²ã€ä¾†æ”¹å¯«å•é¡Œï¼Œä½¿å…¶è®Šå¾—å®Œæ•´ã€‚
    - ä¿æŒå•é¡Œç°¡æ½”ã€‚

    ä¾‹å¦‚ (éœ€è¦æ”¹å¯«):
    å°è©±æ­·å²:
    user: æˆ‘æƒ³æ‰¾æ¸…å¯’çå­¸é‡‘
    assistant: æˆ‘å€‘æœ‰å¹¾ç¨®æ¸…å¯’çå­¸é‡‘ï¼Œä¾‹å¦‚ A å’Œ Bã€‚
    æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œ:
    å®ƒéœ€è¦ä»€éº¼è³‡æ ¼?

    é‡æ§‹å¾Œçš„å•é¡Œ:
    ç”³è«‹ B æ¸…å¯’çå­¸é‡‘éœ€è¦ä»€éº¼è³‡æ ¼ï¼Ÿ

    ä¾‹å¦‚ (ç„¡éœ€æ”¹å¯«):
    å°è©±æ­·å²:
    user: æ…ˆæ¿Ÿé†«ç™‚æ³•äººçåŠ©å­¸é‡‘çš„ç”³è«‹æµç¨‹æ˜¯ä»€éº¼ï¼Ÿ
    assistant: ç”³è«‹æµç¨‹æ˜¯...
    æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œ:
    è¬è¬

    é‡æ§‹å¾Œçš„å•é¡Œ:
    è¬è¬
    """

    user_prompt = f"""å°è©±æ­·å²:
    {history_str}

    æœ€æ–°çš„ä½¿ç”¨è€…å•é¡Œ:
    {question}

    é‡æ§‹å¾Œçš„å•é¡Œ:
    """
    try:
        response = await openai_client.chat.completions.create(
            model=config.OPENAI_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,
            max_tokens=150, # é™åˆ¶è¼¸å‡ºé•·åº¦
        )
        rephrased_question = response.choices[0].message.content.strip()
        # é¿å…è¿”å›ç©ºå­—ä¸²
        if not rephrased_question:
            return question
        print(f"ğŸ”„ é‡æ§‹å¾Œçš„å•é¡Œ: {rephrased_question}")
        return rephrased_question
    except Exception as e:
        print(f"âš ï¸ å•é¡Œé‡æ§‹å¤±æ•—: {e}")
        return question # Fallback to the original question


# --- Streaming Functions ---
# ------------------------------------------------- ç”Ÿæˆç­”æ¡ˆ--------------------------------------------------

async def generate_answer_stream(question: str, cleaned_contexts: list):
    """
    æŠŠæ¸…ç†éçš„ Milvus æª¢ç´¢çµæœäº¤çµ¦ GPT ç”Ÿæˆè‡ªç„¶èªè¨€å›ç­”ï¼Œä¸¦ä»¥ä¸²æµå½¢å¼å›å‚³ã€‚
    é€™æ˜¯ä¸€å€‹ç”Ÿæˆå™¨å‡½å¼ã€‚
    """
    # (The logic for preparing context_for_llm is identical to the non-streaming version)
    from collections import defaultdict
    grouped = defaultdict(list)
    source_url_map = {}
    for c in cleaned_contexts:
        fname = c.get('source_file', 'æœªçŸ¥ä¾†æº')
        grouped[fname].append(c.get('text', ''))
        if fname not in source_url_map and c.get('source_url'):
            source_url_map[fname] = c.get('source_url')

    context_for_llm = ""
    for fname, texts in grouped.items():
        title = fname.replace('.md', '').replace('.txt', '')
        url = source_url_map.get(fname, '')
        context_for_llm += f"\n---\nä¾†æºåç¨±: {title}\n"
        if url:
            context_for_llm += f"ä¾†æºç¶²å€: {url}\n"
        full_text = "\n".join(texts)
        context_for_llm += f"å…§å®¹: {full_text}\n"

    system_prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ…ˆæ¿Ÿå¤§å­¸çå­¸é‡‘å•ç­”åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ã€Œæª¢ç´¢å…§å®¹ã€ä¾†å›ç­”ã€Œä½¿ç”¨è€…å•é¡Œã€ã€‚

    **è¼¸å‡ºæ ¼å¼**
    ä½ çš„è¼¸å‡ºå¿…é ˆåš´æ ¼åŒ…å«å…©éƒ¨åˆ†ï¼Œä¸¦ç”±ä¸€å€‹ç‰¹æ®Šçš„åˆ†éš”ç¬¦è™Ÿ `|||SOURCES|||` éš”é–‹ã€‚

    **ç¬¬ä¸€éƒ¨åˆ†ï¼šçµ¦ä½¿ç”¨è€…çš„å›ç­”**
    1.  **åˆ†æ**ï¼šä»”ç´°åˆ†æã€Œæª¢ç´¢å…§å®¹ã€ï¼Œåˆ¤æ–·å“ªäº›ä¾†æºèˆ‡ã€Œä½¿ç”¨è€…å•é¡Œã€çœŸæ­£ç›¸é—œã€‚
    2.  **ç”Ÿæˆå›ç­”**ï¼š
        * å¦‚æœæœ‰å¤šå€‹çåŠ©å­¸é‡‘ç¨®é¡å°±ç‚ºæ¯å€‹çå­¸é‡‘æˆ–è£œåŠ©å»ºç«‹ä¸€å€‹ç¨ç«‹çš„æ®µè½ã€‚
        * åªå›ç­”å’Œã€Œä½¿ç”¨è€…å•é¡Œã€ç›´æ¥ç›¸é—œçš„è³‡è¨Šï¼Œé¿å…åŒ…å«ä¸ç›¸é—œçš„ç´°ç¯€ã€‚
        * å¦‚æœã€Œæª¢ç´¢å…§å®¹ã€ä¸­æ²’æœ‰ä»»ä½•è³‡è¨Šèƒ½å›ç­”ã€Œä½¿ç”¨è€…å•é¡Œã€ï¼Œè«‹ç¦®è²Œåœ°å‘ŠçŸ¥ä½¿ç”¨è€…ä½ ç„¡æ³•å›ç­”ï¼Œè€Œä¸æ˜¯ç·¨é€ è³‡è¨Šã€‚
        * æ¯å€‹æ®µè½éƒ½å¿…é ˆä»¥åˆ†é»åˆ—å‡ºï¼Œä¸¦å¿…é ˆéµå¾ªä»¥ä¸‹æ ¼å¼ç¨ç«‹å‘ˆç¾ï¼š
            * æ¨™é¡Œï¼šè©²çå­¸é‡‘çš„ã€Œä¾†æºåç¨±ã€ä½œç‚ºæ¨™é¡Œï¼ˆä½¿ç”¨ Markdown çš„ `**ç²—é«”**` æ ¼å¼ï¼‰ã€‚
            * å…§å®¹ï¼šæ ¹æ“šæª¢ç´¢å…§å®¹ä¸­ï¼Œä»¥æµæš¢çš„æ®µè½æˆ–é …ç›®ç¬¦è™Ÿä¾†å‘ˆç¾ã€‚
        * åœ¨æ¨™é¡Œä¸‹æ–¹ï¼Œåƒ…ä½¿ç”¨ç›¸é—œçš„å…§å®¹ä¾†çµ„ç¹”ä½ çš„å›ç­”ã€‚
        * ä½¿ç”¨è‡ªç„¶çš„èªè¨€å’Œ Markdown æ’ç‰ˆï¼ˆç²—é«”ã€é …ç›®ç¬¦è™Ÿç­‰ï¼‰ä¾†ç¾åŒ–è¼¸å‡ºã€‚
    3.  **ç¦æ­¢**ï¼šä¸è¦åœ¨é€™éƒ¨åˆ†åŒ…å«ä»»ä½•é—œæ–¼è³‡æ–™ä¾†æºçš„æ–‡å­—ï¼ˆæ¨™é¡Œé™¤å¤–ï¼‰ã€‚

    **ç¬¬äºŒéƒ¨åˆ†ï¼šè³‡æ–™ä¾†æºåˆ—è¡¨**
    1.  åœ¨åˆ†éš”ç¬¦è™Ÿ `|||SOURCES|||` ä¹‹å¾Œï¼Œä½ å¿…é ˆåˆ—å‡ºä½ åœ¨ç¬¬ä¸€éƒ¨åˆ†å›ç­”ä¸­ï¼Œæ‰€ä½¿ç”¨åˆ°çš„æ‰€æœ‰ã€Œä¾†æºåç¨±ã€ã€‚
    2.  æ ¼å¼ç‚ºä¸€å€‹ç°¡å–®çš„ã€ç”±é€—è™Ÿåˆ†éš”çš„å­—ä¸²ï¼Œä¾‹å¦‚ï¼š`ä¾†æºåç¨±ä¸€,ä¾†æºåç¨±äºŒ`ã€‚
    3.  å¦‚æœæ ¹æ“šã€Œæª¢ç´¢å…§å®¹ã€ç„¡æ³•å›ç­”å•é¡Œï¼Œå‰‡é€™éƒ¨åˆ†æ‡‰ç‚ºç©ºã€‚

    """
    user_prompt = f"""
    ä½¿ç”¨è€…å•é¡Œï¼š
    {question}

    æª¢ç´¢å…§å®¹ï¼š
    {context_for_llm}
    """

    stream = await openai_client.chat.completions.create(
        model=config.OPENAI_MODEL_NAME,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.0,
        stream=True,
    )
    async for chunk in stream:
        content = chunk.choices[0].delta.content or ""
        yield content

async def stream_chat_pipeline(question: str, history: list | None = None):
    """
    Orchestrates the entire RAG pipeline for streaming responses.
    This is an async generator that yields different types of events.
    """
    start_time = time.time()
    full_answer = ""
    original_question = question
    rephrased_question = question
    contexts_for_logging = []
    result_data = {} # Define here to ensure it's available in finally

    try:
        if history:
            rephrased_question = await _rephrase_question_with_history(history, question)
        
        print(f"\nâ“ æœ€çµ‚å•é¡Œ: {rephrased_question} (åŸå§‹: {original_question})")

        intent = await intent_classification(rephrased_question)
        print(f"æ„åœ–: {intent}")

        if intent == "scholarship":
            raw_contexts = await retrieve_context(rephrased_question)
            cleaned_contexts = log_and_clean_contexts(raw_contexts)

            if not cleaned_contexts:
                no_result_answer = "æŠ±æ­‰ï¼Œæˆ‘æ²’æœ‰æ‰¾åˆ°ç›¸é—œçš„è£œåŠ©æˆ–çå­¸é‡‘è³‡è¨Šã€‚"
                yield {"type": "content", "data": no_result_answer}
                full_answer = no_result_answer
                result_data = {"contexts": []} # Set for finally block
                return

            # Stream the answer from LLM, but filter out the source part
            llm_stream = generate_answer_stream(rephrased_question, cleaned_contexts)
            buffer = ""
            delimiter = "|||SOURCES|||"

            # This loop yields the answer part of the stream and stops before the sources.
            # It still accumulates the `full_answer` in the background for parsing later.
            async for chunk in llm_stream:
                full_answer += chunk
                buffer += chunk

                if delimiter in buffer:
                    # Delimiter found. Yield the part before it and stop yielding content.
                    answer_part, _ = buffer.split(delimiter, 1)
                    yield {"type": "content", "data": answer_part}

                    # Consume the rest of the stream without yielding content
                    async for remaining_chunk in llm_stream:
                        full_answer += remaining_chunk
                    break
                else:
                    # To keep the stream flowing, yield parts of the buffer that we know
                    # don't contain the full delimiter. We hold back a small part.
                    if len(buffer) > len(delimiter):
                        yield_part = buffer[:-len(delimiter)]
                        yield {"type": "content", "data": yield_part}
                        buffer = buffer[-len(delimiter):]
            else:
                # If the loop finishes without finding the delimiter, yield any remaining buffer content.
                if buffer:
                    yield {"type": "content", "data": buffer}

            # Parse the full answer to get cited sources
            answer_part = full_answer
            cited_source_names = []
            if "|||SOURCES|||" in full_answer:
                parts = full_answer.split("|||SOURCES|||")
                answer_part = parts[0].strip()
                source_names_str = parts[1].strip()
                if source_names_str:
                    cited_source_names = [name.strip() for name in source_names_str.split(',')]
            
            full_answer = answer_part # Update full_answer to be only the user-facing part

            cited_source_names_set = set(cited_source_names)
            all_cited_contexts = [ctx for ctx in cleaned_contexts if ctx.get('source_file') in cited_source_names_set]
            contexts_for_logging = all_cited_contexts

            unique_display_contexts = []
            seen_keys = set()
            for context in all_cited_contexts:
                unique_key = context.get('source_url') or context.get('source_file')
                if unique_key not in seen_keys:
                    unique_display_contexts.append(context)
                    seen_keys.add(unique_key)
            
            # This is the final payload
            result_data = {"contexts": unique_display_contexts}
        
        else: # Small talk
            stream = await openai_client.chat.completions.create(
                model=config.OPENAI_MODEL_NAME,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ…ˆæ¿Ÿå¤§å­¸çš„èŠå¤©åŠ©ç†ï¼Œä¸»è¦æä¾›çåŠ©å­¸é‡‘å’Œè£œåŠ©è³‡è¨Šã€‚è«‹è‡ªç„¶ä¸”ç°¡çŸ­åœ°å›æ‡‰ï¼Œä¸¦å¼•å°ä½¿ç”¨è€…æå•ç›¸é—œå•é¡Œã€‚è‹¥å•é¡Œç„¡é—œï¼Œè«‹ç¦®è²Œåœ°è¡¨ç¤ºç„¡æ³•å›ç­”ã€‚"},
                    {"role": "user", "content": rephrased_question}
                ],
                temperature=0.7,
                stream=True,
            )
            async for chunk in stream:
                content = chunk.choices[0].delta.content or ""
                full_answer += content
                yield {"type": "content", "data": content}
            
            result_data = {"contexts": []}

    finally:
        end_time = time.time()
        latency_ms = (end_time - start_time) * 1000
        print(f"\nâ±ï¸ æœ¬æ¬¡å•ç­”ç¸½è€—æ™‚: {latency_ms:.2f} ms")
        
        # Log to DB (usage is None for streaming)
        # log_to_db is synchronous; run it in a thread to avoid blocking
        try:
            log_id = await asyncio.to_thread(log_to_db, original_question, rephrased_question, full_answer, contexts_for_logging, latency_ms, None)
        except Exception as e:
            print(f"[ERROR] log_to_db failed in thread: {e}")
            log_id = None

        if log_id:
            result_data["log_id"] = log_id
        
        # Yield the final data packet
        yield {"type": "final_data", "data": result_data}
